{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MonteCarloOffPolicyAgent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgqhbCMt8gm1"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "from collections import defaultdict "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOYl2ftN8nM2"
      },
      "source": [
        "class Environment:\n",
        "  def __init__(self, environment_name):\n",
        "    self.env = gym.make(environment_name)\n",
        "    self.states_number = self.env.observation_space.n\n",
        "    self.actions_number = self.env.action_space.n\n",
        "\n",
        "  def generate_episode(self, policy, max_steps):\n",
        "      state = self.env.reset()\n",
        "      experience = []\n",
        "      for step in range(max_steps):\n",
        "          action = policy.act(state) \n",
        "          new_state, reward, done, info = self.env.step(action)\n",
        "          experience.append((state, action, reward))\n",
        "          state = new_state\n",
        "          if done == True:\n",
        "             break\n",
        "\n",
        "      return experience "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuwvElZdK5s5"
      },
      "source": [
        "class BehaviourPolicy:\n",
        "  def __init__(self, states_number, actions_number):\n",
        "     self.Pi = np.full((states_number, actions_number), 1/ actions_number)\n",
        "    \n",
        "     \n",
        "  def act(self, state):\n",
        "      actions = [a for a in range(len(self.Pi[state]))]\n",
        "      action = random.choices(actions, self.Pi[state, :])[0]\n",
        "      return action\n",
        "\n",
        "  def prob(self, state, action):\n",
        "      return self.Pi[state][action] \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9WQ3C32J0PF"
      },
      "source": [
        "class TargetPolicy:\n",
        "  def __init__(self, states_number, actions_number, Qs):\n",
        "     self.Qs = Qs\n",
        "\n",
        "  def act(self, state):\n",
        "      action = np.argmax(self.Qs[state, :])   \n",
        "      return action\n",
        "\n",
        "  def prob(self, state, action):\n",
        "      max_action = action = np.argmax(self.Qs[state, :])\n",
        "      if action != max_action:\n",
        "         return 0\n",
        "      return 1    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PnNcNDq8s2Q"
      },
      "source": [
        "class MonteCarloOffPolicy:\n",
        "     def __init__(self, env, b_policy):\n",
        "         self.b_policy = b_policy\n",
        "         self.env = env\n",
        "         self.Qs = np.zeros((env.states_number, env.actions_number))\n",
        "         \n",
        "     def train(self, max_episodes = 1600000, max_steps = 100, discount_rate = 0.98):\n",
        "          cumulativeW = np.zeros((env.states_number, env.actions_number)) \n",
        "          for e in range(max_episodes):\n",
        "              episode = env.generate_episode(self.b_policy, max_steps)\n",
        "              G = 0.\n",
        "              W = 1.\n",
        "              for t in reversed(range(0, len(episode))):\n",
        "                  timestep = episode[t]\n",
        "                  state = timestep[0]\n",
        "                  action = timestep[1]\n",
        "                  reward = timestep[2]\n",
        "\n",
        "                  G = discount_rate * G + reward \n",
        "                  if t != len(episode)-1:\n",
        "                     cumulativeW[state][action] += W\n",
        "                     self.Qs[state][action] += (W/cumulativeW[state][action]) * (G - self.Qs[state][action])\n",
        "                     action_max = np.argmax(self.Qs[state, : ])\n",
        "                     \n",
        "                     if action != action_max:\n",
        "                       break\n",
        "                     W *= 1./b_policy.prob(state, action) \n",
        "                 \n",
        "          print(self.Qs)\n",
        "\n",
        "     def act(self, state):\n",
        "        return self.t_policy.act(state) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePEEIfrhNLxt"
      },
      "source": [
        "env = Environment(\"FrozenLake-v0\")\n",
        "\n",
        "b_policy = BehaviourPolicy(env.states_number,  env.actions_number)\n",
        "agent = MonteCarloOffPolicy(env, b_policy)\n",
        "t_policy = TargetPolicy(env.states_number, env.actions_number, agent.Qs)\n",
        "\n",
        "agent.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7V396dViNOl"
      },
      "source": [
        "wins = 0\n",
        "done = False\n",
        "test_env = gym.make(\"FrozenLake-v0\")\n",
        "\n",
        "print(t_policy.Qs)\n",
        "\n",
        "for episode in range(1000):\n",
        "  state = test_env.reset()\n",
        "  done = False\n",
        "\n",
        "  for step in range(100):\n",
        "     action = t_policy.act(state)\n",
        "     new_state, reward, done, info = test_env.step(action) \n",
        "\n",
        "     if done == True:\n",
        "        if reward == 1:\n",
        "           wins +=1\n",
        "        break\n",
        "     state = new_state\n",
        "        \n",
        "print(\"wins ratio \", wins/1000) \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
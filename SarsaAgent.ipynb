{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SarsaAgent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBvPsbJfuMD2"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "from collections import defaultdict "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnHhI5uRukcY"
      },
      "source": [
        "class SarsaAgent:\n",
        "     def __init__(self, env):\n",
        "         self.env = env\n",
        "         self.Qs = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "    \n",
        "     def getAction(self, state, exploration_rate):\n",
        "         exploration_rate_threshold = random.uniform(0,1)\n",
        "         if exploration_rate_threshold > exploration_rate:\n",
        "            action = np.argmax(self.Qs[state,:])\n",
        "         else:\n",
        "            action = env.action_space.sample()\n",
        "         return action\n",
        "\n",
        "     def train(self, max_episodes = 15000, max_steps = 100, discount_rate = 0.98, learning_rate = 0.1, \n",
        "              exploration_rate_max = 1., exploration_rate_min = 0.01, exploration_decay_rate = 0.001):\n",
        "      \n",
        "          exploration_rate = 1\n",
        "\n",
        "          for e in range(max_episodes):\n",
        "             state = env.reset()\n",
        "             done = False\n",
        "             action = self.getAction(state, exploration_rate)\n",
        "\n",
        "             for step in range(max_steps):\n",
        "                new_state, reward, done, _ = env.step(action) \n",
        "                if done == True:\n",
        "                   self.Qs[state, action] = self.Qs[state, action] + \\\n",
        "                     learning_rate *(reward - self.Qs[state, action])\n",
        "                   break\n",
        "                new_action = self.getAction(new_state, exploration_rate) \n",
        "\n",
        "                self.Qs[state, action] = self.Qs[state, action] + \\\n",
        "                    learning_rate *(reward + discount_rate * self.Qs[new_state, new_action] - self.Qs[state, action])\n",
        "\n",
        "                state = new_state\n",
        "                action = new_action\n",
        "\n",
        "                if done == True:\n",
        "                   break\n",
        "  \n",
        "                exploration_rate = exploration_rate_min + (exploration_rate_max - exploration_rate_min) * np.exp(-exploration_decay_rate * e)    \n",
        "     \n",
        "          print(self.Qs)\n",
        "\n",
        "     def act(self, state):\n",
        "        return np.argmax(self.Qs[state,:]) "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvVJCMlYxAqM",
        "outputId": "372c17af-0753-4d75-8ee5-0dfd08b66b3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env = gym.make(\"FrozenLake-v0\")\n",
        "\n",
        "agent = SarsaAgent(env)\n",
        "agent.train()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.38202568 0.3042228  0.28493387 0.30210653]\n",
            " [0.18341057 0.16928523 0.15947324 0.32604241]\n",
            " [0.17625244 0.17959803 0.18313258 0.24410041]\n",
            " [0.1084419  0.10262964 0.08574089 0.22852448]\n",
            " [0.40598877 0.25074169 0.24046969 0.27965886]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.22574918 0.08483758 0.0916693  0.07821771]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.25362223 0.30115139 0.282066   0.46718855]\n",
            " [0.3190801  0.51572764 0.36332983 0.21459444]\n",
            " [0.55505282 0.26951307 0.20798134 0.2380352 ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.34189075 0.39539594 0.68634864 0.45037999]\n",
            " [0.59582279 0.79885226 0.63181664 0.62331125]\n",
            " [0.         0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm1bRkFyxvwp",
        "outputId": "059007a8-2991-4039-d934-7862148bd09b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "wins = 0\n",
        "done = False\n",
        "test_env = gym.make(\"FrozenLake-v0\")\n",
        "\n",
        "for episode in range(1000):\n",
        "  state = test_env.reset()\n",
        "  done = False\n",
        "\n",
        "  for step in range(100):\n",
        "     action = agent.act(state)\n",
        "     new_state, reward, done, info = test_env.step(action) \n",
        "\n",
        "     if done == True:\n",
        "        if reward == 1:\n",
        "           wins +=1\n",
        "        break\n",
        "     state = new_state\n",
        "        \n",
        "print(\"wins ratio \", wins/1000) "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wins ratio  0.763\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MonteCarloOnPolicyAgent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgqhbCMt8gm1"
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "from collections import defaultdict "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOYl2ftN8nM2"
      },
      "source": [
        "class Environment:\n",
        "  def __init__(self, environment_name):\n",
        "    self.env = gym.make(environment_name)\n",
        "    self.states_number = self.env.observation_space.n\n",
        "    self.actions_number = self.env.action_space.n\n",
        "\n",
        "  def generate_episode(self, policy, max_steps):\n",
        "      state = self.env.reset()\n",
        "      \n",
        "      experience = []\n",
        "      for step in range(max_steps):\n",
        "          action = policy.act(state, training = True) \n",
        "          new_state, reward, done, info = self.env.step(action)\n",
        "          experience.append((state, action, reward))\n",
        "          state = new_state\n",
        "          if done == True:\n",
        "             break\n",
        "\n",
        "      return experience "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuwvElZdK5s5"
      },
      "source": [
        "class Policy:\n",
        "  def __init__(self, states_number, actions_number):\n",
        "     self.Pi = np.full((states_number, actions_number), 1/ actions_number)\n",
        "\n",
        "  def act(self, state, training = False):\n",
        "      if training == True:\n",
        "         actions = [a for a in range(len(self.Pi[state]))]\n",
        "         action = random.choices(actions, self.Pi[state, :])[0]\n",
        "      else:\n",
        "         action = np.argmax(self.Pi[state, :])   \n",
        "      return action\n",
        "\n",
        "  def update(self, state, action, value):\n",
        "      self.Pi[state][action] = value "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PnNcNDq8s2Q"
      },
      "source": [
        "class MonteCarloOnPolicy:\n",
        "     def __init__(self, env, policy):\n",
        "         self.policy = policy\n",
        "         self.env = env\n",
        "         self.Qs = np.zeros((env.states_number, env.actions_number))\n",
        "\n",
        "     def train(self, max_episodes = 100000, max_steps = 100, discount_rate = 0.98, epsilon = 0.3):\n",
        "          returns = defaultdict(list)  \n",
        "          for e in range(max_episodes):\n",
        "              episode = env.generate_episode(self.policy, max_steps)\n",
        "              G = 0\n",
        "              count =0\n",
        "              for i in reversed(range(0, len(episode))):\n",
        "                  timestep = episode[i]\n",
        "                  state = timestep[0]\n",
        "                  action = timestep[1]\n",
        "                  reward = timestep[2]\n",
        "                  G = discount_rate * G + reward \n",
        "      \n",
        "                  if not (state, action) in [(t[0],t[1]) for t in episode[0:i]]:\n",
        "                     returns[(state, action)].append(G)\n",
        "                     self.Qs[state][action] = np.mean(np.array(returns[(state, action)]))\n",
        "                     max_action = np.argmax(self.Qs[state, : ])\n",
        "\n",
        "                     for action in range(self.env.actions_number):\n",
        "                         if max_action == action:\n",
        "                            self.policy.update(state, action, 1 - epsilon + (epsilon / self.env.actions_number)) \n",
        "                         else:\n",
        "                            self.policy.update(state, action, epsilon / self.env.actions_number)\n",
        "     \n",
        "          print(self.Qs)\n",
        "\n",
        "     def act(self, state):\n",
        "        return self.policy.act(state) \n",
        "\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePEEIfrhNLxt",
        "outputId": "b2ad411d-22dc-4898-e7d7-79287010f585",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env = Environment(\"FrozenLake-v0\")\n",
        "\n",
        "policy = Policy(env.states_number, env.actions_number)\n",
        "agent = MonteCarloOnPolicy(env, policy)\n",
        "agent.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.10454445 0.09559594 0.09889683 0.09149859]\n",
            " [0.05748951 0.06436491 0.05930372 0.09048444]\n",
            " [0.10032586 0.0910475  0.09902385 0.08661042]\n",
            " [0.05527071 0.05460277 0.04963884 0.0804791 ]\n",
            " [0.12312643 0.09117876 0.08645882 0.0714447 ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.13644591 0.10109804 0.12867966 0.02932373]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.08976574 0.14039456 0.12540382 0.1786621 ]\n",
            " [0.18812248 0.28682332 0.23531673 0.15607385]\n",
            " [0.33883578 0.30443626 0.24591092 0.12899032]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.21812244 0.34698722 0.43188239 0.30607072]\n",
            " [0.45671521 0.68134188 0.65373953 0.56391634]\n",
            " [0.         0.         0.         0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7V396dViNOl",
        "outputId": "8a45c06b-7c5c-4ed1-dc76-d4180a5291ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "wins = 0\n",
        "done = False\n",
        "test_env = gym.make(\"FrozenLake-v0\")\n",
        "\n",
        "for episode in range(1000):\n",
        "  state = test_env.reset()\n",
        "  done = False\n",
        "\n",
        "  for step in range(100):\n",
        "     action = agent.act(state)\n",
        "     new_state, reward, done, info = test_env.step(action) \n",
        "\n",
        "     if done == True:\n",
        "        if reward == 1:\n",
        "           wins +=1\n",
        "        break\n",
        "     state = new_state\n",
        "        \n",
        "print(\"wins ratio \", wins/1000) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wins ratio  0.713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEJrtcrx2vw9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnYzvwvMNOBV"
      },
      "source": [
        "Comparison"
      ]
    }
  ]
}